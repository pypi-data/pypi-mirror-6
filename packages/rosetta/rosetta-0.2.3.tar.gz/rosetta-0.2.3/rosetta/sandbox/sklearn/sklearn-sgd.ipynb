{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
      "from sklearn import cross_validation\n",
      "\n",
      "from rosetta.text import streamers, nlp, text_processors\n",
      "from rosetta.modeling import categorical_fitter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The CountVectorizer converts iterables over strings to a \"vectorizer\", that \n",
      "# is like a Gensim dictionary and more.  It has a built in tokenizer, and\n",
      "# you can over-ride it.  It does everything that rosetta does, except stream\n",
      "# text files.\n",
      "vectorizer = CountVectorizer(stop_words='english', max_df=1.0, min_df=1, decode_error='ignore')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Corpus should be interable over strings\n",
      ">>> corpus = [\n",
      "...     'This is the first document about cats.',\n",
      "...     'This is the second second document about dogs.',\n",
      "...     'And the third one about god.',\n",
      "...     'this is the fourth document about jesus',\n",
      "... ]\n",
      "\n",
      "# y is the target\n",
      "y = [1, 1, 0, 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.fit(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error='ignore',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "{u'cats': 0,\n",
        " u'document': 1,\n",
        " u'dogs': 2,\n",
        " u'fourth': 3,\n",
        " u'god': 4,\n",
        " u'jesus': 5,\n",
        " u'second': 6}"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is the vectorizer method to transform a document (string) into\n",
      "# a sparse array.  We call .toarray() to convert to a dense array.\n",
      "vectorizer.transform(['hello and first second third']).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "array([[0, 0, 0, 0, 0, 0, 1]])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = SGDClassifier(loss='log', penalty='l1', alpha=0.1, shuffle=True, n_iter=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "SGDClassifier(alpha=0.1, class_weight=None, epsilon=0.1, eta0=0.0,\n",
        "       fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
        "       loss='log', n_iter=50, n_jobs=1, penalty='l1', power_t=0.5,\n",
        "       random_state=None, rho=None, shuffle=True, verbose=0,\n",
        "       warm_start=False)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "array([1, 1, 0, 0])"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Use with `rosetta`\n",
      "\n",
      "The only added functionality I see is the `TextFileStreamer`.  We use `streamer.single_stream('text')` in place of the above `corpus`.  This is fine, but now we have two separate workflows...the `sklearn` workflow uses our streamer then sklearn for feature selection.  The VW workflow uses our streamer then `SFileFilter` for variable selection.  `SFileFilter` is nice since it works with `pandas`, but it's sort of non-standard..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "streamer = streamers.TextFileStreamer(\n",
      "    text_base_path='/home/langmore/data/prod/kiss-01/raw/bodyfiles/',\n",
      "    tokenizer=text_processors.TokenizerBasic())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(\n",
      "    stop_words='english', max_df=0.9, min_df=10, decode_error='ignore')\n",
      "vectorizer.fit(streamer.single_stream('text', cache_list=['doc_id']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error='ignore',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=0.9, max_features=None, min_df=10,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.fit_transform(streamer.single_stream('text', cache_list=['doc_id']))\n",
      "y = pd.Series(np.random.randint(0, 2, len(streamer.doc_id)), index=streamer.doc_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = SGDClassifier(loss='log', penalty='l1', alpha=0.00001, shuffle=True, n_iter=5, n_jobs=-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(X, y.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "SGDClassifier(alpha=1e-05, class_weight=None, epsilon=0.1, eta0=0.0,\n",
        "       fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
        "       loss='log', n_iter=5, n_jobs=-1, penalty='l1', power_t=0.5,\n",
        "       random_state=None, rho=None, shuffle=True, verbose=0,\n",
        "       warm_start=False)"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Cross validate\n",
      "yscore = categorical_fitter.predict_proba_cv(clf, X, y.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cross_validation.cross_val_score(clf, X, y.values, scoring='roc_auc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "array([ 0.49777565,  0.50338768,  0.48803199])"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}