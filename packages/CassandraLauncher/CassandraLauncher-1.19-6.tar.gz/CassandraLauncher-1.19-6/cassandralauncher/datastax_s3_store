#!/usr/bin/env python

import ConfigParser
import os
import re
import shlex
import subprocess
import yaml

configfile = '/home/ubuntu/.s3cfg'

def exe(command, wait=True):
    """Execute a subprocess command"""

    # Open a subprocess to run your command
    process = subprocess.Popen(shlex.split(str(command)), stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if wait:
        read = process.communicate()
        return read
    else:
        return process

def s3_store():
    print "Draining the node..."
    exe('nodetool -h localhost drain')


    # Create the bucket
    exe('s3cmd mb s3://%s-%s' % (bucket_name, access_key))

    print "Uploading data..."
    response = exe('sudo s3cmd sync --delete-removed %s s3://%s-%s/%s/%s/' % (
        os.path.join(root_data_dir, 'data/'),
        bucket_name, access_key,
        cluster_name, initial_token
    ))
    if response[1]:
        print response[1]

    print "Stopping the node..."
    exe('sudo service dse stop')
    exe('sudo service cassandra stop')


# Read access_key
config = ConfigParser.RawConfigParser()
config.read(configfile)
access_key = config.get('default', 'access_key')
bucket_name = config.get('datastax', 'bucket_name') if config.has_option('default', 'bucket_name') else 'datastax_s3_storage'
root_data_dir = config.get('datastax', 'root_data_dir') if config.has_option('default', 'root_data_dir') else '/raid0/cassandra'

# Read cluster_name and initial_token
with open('/etc/dse/cassandra/cassandra.yaml') as f:
    dataMap = yaml.load(f)
cluster_name = re.sub(r'\W+', '_', dataMap['cluster_name'])
initial_token = dataMap['initial_token']

s3_store()
